setwd("D:/2020年summer/visualization/8/dm4_data-master/data")
library(tm)
library(grid)
library(wordcloud)
library(wordcloud2)
library(tidyverse)
install.packages("tm")
install.packages("wordcloud")
install.packages("wordcloud2")
library(tm)
library(grid)
library(wordcloud)
library(wordcloud2)
library(tidyverse)
docs <- VCorpus(DirSource(texts))
summary(docs)
texts <- file.path("D:/2020年summer/visualization/8/dm4_data-master/data", "texts")
dir(texts)
docs <- VCorpus(DirSource(texts))
summary(docs)
writeLines(as.character(docs[1]))
docs <- tm_map(docs, removePunctuation)
writeLines(as.character(docs[1]))
for (j in seq(docs)) {
docs[[j]] <- gsub("/", " ", docs[[j]])
docs[[j]] <- gsub("’", " ", docs[[j]])
docs[[j]] <- gsub("—", " ", docs[[j]])
docs[[j]] <- gsub("\\|", " ", docs[[j]])
docs[[j]] <- gsub("@", " ", docs[[j]])
docs[[j]] <- gsub("\u2028", " ", docs[[j]])  # an ascii character that might not translate
}
writeLines(as.character(docs[1])) # always inspect
for (j in seq(docs)) {
docs[[j]] <- gsub("fake news", "fake_news", docs[[j]])
docs[[j]] <- gsub("inner city", "inner-city", docs[[j]])
docs[[j]] <- gsub("I m", "I'm", docs[[j]])
docs[[j]] <- gsub("politically correct", "politically_correct", docs[[j]])
}
docs <- tm_map(docs, PlainTextDocument)
View(docs)
docs <- tm_map(docs,
removeWords,
stopwords("english"))
docs <- tm_map(docs, PlainTextDocument)
docs <- tm_map(docs, removeWords, c("will"))
docs <- tm_map(docs, stripWhitespace)
docs <- tm_map(docs, PlainTextDocument)
dtm <- DocumentTermMatrix(docs)
tdm <- TermDocumentMatrix(docs)
View(dtm)
frequency <- sort(colSums(as.matrix(dtm)),
decreasing=TRUE)
head(frequency)
findFreqTerms(dtm, lowfreq = 100)
frequency <- sort(colSums(as.matrix(dtm)),
decreasing=TRUE)
head(frequency)
findFreqTerms(dtm, lowfreq = 100)
wf <- data.frame(word = names(frequency),
freq = frequency)
head(wf, 15)
findAssocs(dtm,
terms = "great",
corlimit = 0.90)
findAssocs(dtm,
terms = c("great" , "america"),
corlimit = 0.90)
findAssocs(dtm,
terms = "hell",
corlimit = c(0.75, 0.5))
library(aRxiv) # API interface
library(lubridate) # for time measures
library(tidyverse)
library(skimr)
library(wordcloud)
install.packages("aRxiv")
library(tidyverse)
library(tidytext)
library(tm)
library(sotu)
library(igraph) # for graph_from_data_frame function
library(ggraph) # for network plot
library(quanteda)
library(wordcloud)
library(topicmodels)
install.packages("ggraph")
install.packages("sotu")
install.packages("quanteda")
install.packages("topicmodels")
library(tidyverse)
library(tidytext)
library(tm)
library(sotu)
library(igraph) # for graph_from_data_frame function
library(ggraph) # for network plot
library(quanteda)
library(wordcloud)
library(topicmodels)
install.packages("tidytext")
library(tidyverse)
library(tidytext)
library(tm)
library(sotu)
library(igraph) # for graph_from_data_frame function
library(ggraph) # for network plot
library(quanteda)
library(wordcloud)
library(topicmodels)
sotu_meta$text <- sotu_text
head(sotu_meta)
texts<-sotu_meta%>%
filter(year>1929 and year<1945)%>%
as.tibble()
texts<-sotu_meta%>%
filter(year>1929 and year<1945)
texts<- sotu_meta %>%
filter(year>1929,year<1945)
head(texts)
library(tibble)
library(textdata)
library(tidytext)
install.packages("textdata")
glimpse(get_sentiments("bing"))
head(texts)
texts<- sotu_meta %>%
filter(year>1955)
docs <- VCorpus(texts[,'text'])
texts[,'text']
texts<- sotu_meta %>%
filter(year>1955)%>%
select(text)
head(texts)
texts <- tm_map(texts, removePunctuation)
(unt_sotu <- sotu_meta %>%
unnest_tokens(output = word,
input = text))
as_tibble(stop_words)
stops_sotu <- unt_sotu %>%
anti_join(stop_words,
by = "word")
head(unt_sotu)
dim(unt_sotu)
dim(sotu_meta)
get_sentiments("oh")
sadness <- get_sentiments("nrc") %>%
filter(sentiment == "sadness")
sadness <- get_sentiments("bing") %>%
filter(sentiment == "sadness")
texts %>%
inner_join(sadness) %>%
count(word,
sort = TRUE)
sadness
texts<- unt_sotu %>%
filter(year>1929,year<1945)
get_sentiments("bing")
sadness <- get_sentiments("bing") %>%
filter(sentiment == "sadness")
texts %>%
inner_join(sadness) %>%
count(word,
sort = TRUE)
head(unt_sotu)
dim(unt_sotu)
(unt_sotu <- sotu_meta %>%
unnest_tokens(output = word,
input = text))
as_tibble(stop_words)
stops_sotu <- unt_sotu %>%
anti_join(stop_words,
by = "word")
dim(unt_sotu)
texts<- unt_sotu %>%
filter(year>1955)
head(texts)
new<- sotu_meta %>%
filter(year>1955)
head(new)
(unt_sotu <- new %>%
unnest_tokens(output = word,
input = text))
(unt_sotu <- new %>%
unnest_tokens(output = word,
input = text))
as_tibble(stop_words)
stops_sotu <- unt_sotu %>%
anti_join(stop_words,
by = "word")
dim(stops_sotu)
head(stops_sotu)
dtm <- tidy(AssociatedPress) %>%
count(document, word, wt = count) %>%
spread(word, n, fill = 0) %>%
arrange(word)
dtm <- tidy(unt_sotu) %>%
count(document, word, wt = count) %>%
spread(word, n, fill = 0) %>%
arrange(word)
cast_dtm(unt_sotu,word,president)
cast_dtm(unt_sotu,word,document, weighting = tm::weightTf)
cast_dtm(unt_sotu,word,president, weighting = tm::weightTf)
head(unt_sotu)
aa<-cast_dtm(unt_sotu,word,president, weighting = tm::weightTf)
aa<-cast_dtm(sotu_meta,word,president, weighting = tm::weightTf)
aa<-cast_dtm(unt_sotu,word,president, weighting = tm::weightTf)
unt_sotu
aa<-cast_dtm(unt_sotu,word,president, weighting = tm::weightTf)
aa<-cast_dtm(unt_sotu,president, word,weighting = tm::weightTf)
aa<-cast_dtm(unt_sotu,word,president,weighting = tm::weightTf)
modern <- sotu_meta %>%
filter(year > 1955) %>%
unnest_tokens(word, text) %>%
anti_join(stop_words, by = "word") %>%
group_by(year) %>%
count(word)
dtm <- cast_dtm(modern, year, word, n)
View(dtm)
